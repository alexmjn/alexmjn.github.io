---
title: Manually Implementing a Naive Bayes Classifier
---

# A From-Scratch Implementation of Naive Bayes — Pt 1

(See github repo for full code.)

One of the most important things someone looking to understand machine learning should do is to design a model class from scratch. It might not seem important: “When will I ever use this when I can just call an optimized scikit-learn model that abstracts away those details?” But all models are [leaky abstractions](https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/): 95% of the time they’ll work perfectly, while the 5% of the time you need to get something from it you’ll be glad you have an understanding of what’s going on under the hood.

Your personal implementation doesn’t need to be perfect; it’s not production code (and my code is quite ugly in some spots). But the process of getting it to work gives you hands-on experience in how these implementations work. And having this experience will help you to dive into source code of production implementations for an even deeper understanding. In this post, I’ll walk through a bare-bones implementation of Naive Bayes classification with a Gaussian feature distribution.

The Naive Bayes algorithm is named after its grounding in Bayesian inference. A traditional example is in medical testing: we might want to know what the chances are that a patient has COVID-19 given that they tested positive. Bayes’ Theorem reduces this problem to “What is the probability a person tests positive given they have COVID-19?” multiplied by “What is the probability a random person has COVID-19?” This naturally translates into the language of machine learning. We have training data with features; we have a group of target classes we want to predict; we want to decide, given a specific set of features, which target class is most likely. To do that, we go class by class — What’s the likelihood of these features if the class is true? How frequently is this class true? — and return the highest probability as our prediction.

Let’s imagine we’re trying to guess whether a house is “expensive” (>$500,000) based on its number of bedrooms, size, and whether it has a pool. To get a formal Bayesian prediction of whether a large 5-bedroom house with a pool is expensive, we’d need to take into account the relationships between these variables. However, intuitively, we can guess that combining the features in isolation will often yield the correct answer even without taking this into account. This is the “naive” part of the algorithm, which drastically simplifies calculations: the assumption that the features we observe do not affect one another. As you might expect, this assumption is rarely true in practice ([everything is correlated](https://www.gwern.net/Everything)); yet, empirically, the algorithm remains surprisingly useful.

In beginning the implementation, we need a method of attack. Machine learning models are traditionally written using an object-oriented paradigm, so we’ll start by defining a class. This provides a method to organize our algorithm, prevents code re-use, and makes things easier on the end user. Instead of needing to track a bunch of parameters and arguments, those will simply attach to the model as attributes to be used later in the pipeline. Once hyperparameters are set as the model is instantiated, future uses of that model will automatically pull from those hyperparameters without needing to call them explicitly. OOP is what lets the stereotypical model pipeline: model = Model(), model.fit(), model.predict()work so cleanly.

    class NaiveBayes:
      def __init__(self, distribution="Gaussian"):
        self.distribution=distribution
        self.classes = []
        self.class_freqs = []
        self.means_sds = []

Before we fit or predict any data, we want an initialization function that takes in any hyperparameters. As it turns out, Naive Bayes is relatively simple to initialize. The main decision is the choice of statistical distributions that we want to assume for our features; here, we pick a normal (or Gaussian) distribution, which is commonly used for continuous features. Empty lists are initialized to hold the outcomes of fitting this model to actual data.

In order to “fit” our model, we need to unpack what the algorithm wants to do. To do that, let’s look at a formal definition of how the normal distribution is used in Naive Bayes. The following equation may look complex, but it’s understandable, and it’s going to be at the core of our implementation.

![The proof is left as an exercise for the reader.](https://cdn-images-1.medium.com/max/2000/1*00Pga2nl0gio7xAs0JFqpQ.png)*The proof is left as an exercise for the reader.*

This is surprisingly easy to translate into code. The equation can be summarized as: given a specific class (*Cₖ*), the probability of observing a specific value (*v*) of a feature (*x*) of that class depends on how far *v* is from the sample mean for that class (**μ**ₖ) and the sample variance for that class (σ²ₖ). Basically: to predict whether someone six feet tall is male, we need to find out the average and variation of male height, plus the average and variation of female height. These numbers will come from our training data — this is “training” a Naive Bayes model. Below I’ve defined a little helper function that calculates this equation: given a specific feature observation (six feet) plus the average and standard deviation of that feature, it’ll pop out the probability density we’re looking for.

    def gaussian_numerator(obs, mean, sd):
      coefficient = 1 / ((2 * math.pi * sd**2)**(1/2))
      exponent = math.exp((-(obs - mean)**2)/(2 * sd**2))

      return coefficient * exponent

Once we have this, we can independently calculate “how likely is a male to be six feet given our distribution” and “how likely is a female to be six feet given our distribution”, and the ratio of those two probability densities is the ratio of “evidence” this observed feature contributes toward our prediction.

The final piece of the puzzle is our “prior”, in Bayesian terms: basically, we want to bias our model to guess things that are more frequent in the real world. A ransacked cabin might be more consistent with yeti behavior than human behavior, but there are a lot more humans than yetis in the world. So, for this implementation, we’ll simply look at how frequent each target class was in the training data, and multiply each probability by that as our Bayesian prior/yeti-scaling factor.

We’ve discussed what Naive Bayes is doing and the information we’ll need to calculate from our training data in order to make actual predictions. Now that we’ve built up some scaffolding, we’ll get down to making everything work.

# A From-Scratch Implementation of Naive Bayes — Pt 2

(see part 1)

We’ve explored how the Naive Bayes model works in theory and the framework for programming it. Now we move on to fitting the model. As discussed previously, the “fitted model” for Naive Bayes consists of a set of distributions. For each feature, we need to break the data up by class, then find the class-specific mean and standard deviation, so that we can determine which distribution our actual observation is more likely to have come from. Let’s work on putting that into code. (Because these are familiar, I’ve used the pandas and numpy libraries to help. A “true” from-scratch implementation might avoid these, but I’m happy peeking under one hood at a time.)

If this seems abstract, an example is below: we split up our labeled training data by class, then calculate the feature characteristics.

![Proprietary graphics software, DM me for details.](https://cdn-images-1.medium.com/max/8064/1*JLIPCFQe0VZT-HfwH9I5CA.png)*Proprietary graphics software, DM me for details.*

    def fit(self, X, y):
      n = X.shape[0]
      self.classes = y.unique()
      X["target"] = y

On to the code: to calculate the sample variance, we’ll need the sample size, which is the first dimension of the data frame passed in. To split the data by class, we parse the target data frame for all the unique class values, then attach that to our observations.

      for class_type in classes:
        df_subclass = X[X["target"] == class_type]
        class_freq = df_subclass.shape[0] / n
        self.class_freqs.append(class_freq)

        df_subclass = df_subclass.drop(["target"], axis=1)
        subclass_stds = df_subclass.std()
        subclass_means = df_subclass.mean()
        means_sds = pd.concat([subclass_means, subclass_stds], axis=1)
        self.class_means_sds.append(means_sds)

      return

Breaking this down: for each class, we separate out that class’s data, then find the frequency of that class. This will be our Bayesian prior/yeti factor. Once we’ve separated the data frame by class, we don’t need the class any more, so we throw that back out. We use built-in Pandas methods to calculate the standard deviations and means (although this would be relatively trivial to calculate by hand). Finally, we package that data up and attach it to the class — a benefit of object-oriented programming. After all this, we can simply return; since the function persistently alters the class object, we have no need to pass any value out of the function.

Once we’ve used our training data to fit our `Model()` object with all the bells and whistles, it’s time to use it to predict.

    def predict(self, X, y):
      predictions = []

      for i in range(y.shape[0]):
        class_preds = []

        for j in range(len(self.classes)):
          probs = self.class_freqs[j]

          for k in range(X.shape[1]):
            obs = X.iloc[i][k]
            mean = self.means_sds[j][0][k]
            sd = self.means_sds[j][1][k]
            probs = probs * gaussian_numerator(obs, mean, sd)
          class_preds.append(probs)

        max_class_index = class_preds.index(max(class_preds))
        predictions.append(self.classes[max_class_index])
        print(class_preds)

      return pd.Series(predictions)

![haha nested for loop go brrrr](https://cdn-images-1.medium.com/max/2000/1*A4LPmCUutGlvWmR_rMeBUQ.png)*haha nested for loop go brrrr*

This is the hackiest bit of code here, but it fits the intuition of the model. The inner loop a) pulls the feature observation b) finds the mean and standard deviation of that feature for the current class and c) calculates the likelihood of the observation given those things. Because of our “naive” assumption, we’re able to multiply all those probabilities together to get one probability of observing all the features for each class. Finally, we take the maximum of this set of probabilities to obtain our prediction, then add that prediction on to our predictions list.

The outer loop simply repeats this process for each row in the data passed in so that multiple predictions can be made at the same time.

I’m stopping this class here, but there are plenty of bells and whistles to add on. For example, we have no accuracy metric right now. We might also want to create a set of “assert” statements to make sure our input is valid. A test suite could be used to make sure any changes don’t break our code. This is just dipping our toes in the water!

Communication of knowledge is the ultimate test of true understanding. Writing and programming this helped me gain a clearer understanding of Naive Bayes and implementing a machine learning algorithm “by hand” in Python, and I hope it helps readers as much as it helped me.
