---
title: Manually Implementing a Naive Bayes Classifier
---

# A From-Scratch Implementation of Naive Bayes

(See github repo for full code.)

One of the most important things someone looking to understand machine learning should do is to design a model class from scratch. It might not seem important: “When will I ever use this when I can just call an optimized scikit-learn model that abstracts away those details?” But all models are leaky abstractions: 95% of the time they’ll work perfectly, while the 5% of the time you need to get something from it you’ll be glad you have an understanding of what’s going on under the hood.
Your personal implementation doesn’t need to be perfect; it’s not production code (and my code is quite ugly in some spots). But the process of getting it to work gives you hands-on experience in how these implementations work. And having this experience will help you to dive into source code of production implementations for an even deeper understanding. In this post, I’ll walk through a bare-bones implementation of Naive Bayes classification with a Gaussian feature distribution.
The Naive Bayes algorithm is named after its grounding in Bayesian inference. A traditional example is in medical testing: we might want to know what the chances are that a patient has COVID-19 given that they tested positive. Bayes’ Theorem reduces this problem to “What is the probability a person tests positive given they have COVID-19?” multiplied by “What is the probability a random person has COVID-19?” This naturally translates into the language of machine learning. We have training data with features; we have a group of target classes we want to predict; we want to decide, given a specific set of features, which target class is most likely. To do that, we go class by class — What’s the likelihood of these features if the class is true? How frequently is this class true? — and return the highest probability as our prediction.
Let’s imagine we’re trying to guess whether a house is “expensive” (>$500,000) based on its number of bedrooms, size, and whether it has a pool. To get a formal Bayesian prediction of whether a large 5-bedroom house with a pool is expensive, we’d need to take into account the relationships between these variables. However, intuitively, we can guess that combining the features in isolation will often yield the correct answer even without taking this into account. This is the “naive” part of the algorithm, which drastically simplifies calculations: the assumption that the features we observe do not affect one another. As you might expect, this assumption is rarely true in practice (everything is correlated); yet, empirically, the algorithm remains surprisingly useful.
In beginning the implementation, we need a method of attack. Machine learning models are traditionally written using an object-oriented paradigm, so we’ll start by defining a class. This provides a method to organize our algorithm, prevents code re-use, and makes things easier on the end user. Instead of needing to track a bunch of parameters and arguments, those will simply attach to the model as attributes to be used later in the pipeline. Once hyperparameters are set as the model is instantiated, future uses of that model will automatically pull from those hyperparameters without needing to call them explicitly. OOP is what lets the stereotypical model pipeline: model = Model(), model.fit(), model.predict()work so cleanly.
class NaiveBayes:
  def __init__(self, distribution="Gaussian"):
    self.distribution=distribution
    self.classes = []
    self.class_freqs = []
    self.means_sds = []
Before we fit or predict any data, we want an initialization function that takes in any hyperparameters. As it turns out, Naive Bayes is relatively simple to initialize. The main decision is the choice of statistical distributions that we want to assume for our features; here, we pick a normal (or Gaussian) distribution, which is commonly used for continuous features. Empty lists are initialized to hold the outcomes of fitting this model to actual data.
In order to “fit” our model, we need to unpack what the algorithm wants to do. To do that, let’s look at a formal definition of how the normal distribution is used in Naive Bayes. The following equation may look complex, but it’s understandable, and it’s going to be at the core of our implementation.

The proof is left as an exercise for the reader.
This is surprisingly easy to translate into code. The equation can be summarized as: given a specific class (Cₖ), the probability of observing a specific value (v) of a feature (x) of that class depends on how far v is from the sample mean for that class (μₖ) and the sample variance for that class (σ²ₖ). Basically: to predict whether someone six feet tall is male, we need to find out the average and variation of male height, plus the average and variation of female height. These numbers will come from our training data — this is “training” a Naive Bayes model. Below I’ve defined a little helper function that calculates this equation: given a specific feature observation (six feet) plus the average and standard deviation of that feature, it’ll pop out the probability density we’re looking for.
def gaussian_numerator(obs, mean, sd):
  coefficient = 1 / ((2 * math.pi * sd**2)**(1/2))
  exponent = math.exp((-(obs - mean)**2)/(2 * sd**2))
  return coefficient * exponent
Once we have this, we can independently calculate “how likely is a male to be six feet given our distribution” and “how likely is a female to be six feet given our distribution”, and the ratio of those two probability densities is the ratio of “evidence” this observed feature contributes toward our prediction.
The final piece of the puzzle is our “prior”, in Bayesian terms: basically, we want to bias our model to guess things that are more frequent in the real world. A ransacked cabin might be more consistent with yeti behavior than human behavior, but there are a lot more humans than yetis in the world. So, for this implementation, we’ll simply look at how frequent each target class was in the training data, and multiply each probability by that as our Bayesian prior/yeti-scaling factor.
We’ve discussed what Naive Bayes is doing and the information we’ll need to calculate from our training data in order to make actual predictions. Now that we’ve built up some scaffolding, we’ll get down to making everything work in part 2.
[I wrote up analysis in more detail here.](https://medium.com/@ajenkneary/clearing-out-crime-4b1267f7274d).

## Code

Code, analysis, and commentary can be found in [This Jupyter Notebook](https://github.com/alexmjn/Predicting-Arrests/blob/master/Crime_Data.ipynb).
Analysis can be replicated using the data in [the repo](https://github.com/alexmjn/Predicting-Arrests).
